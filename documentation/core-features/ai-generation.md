# AI Generation - Multi-Modal Creation

OPSIIE provides multi-modal AI generation capabilities for images, videos, and music using various AI models and APIs.

## üé® Overview

AI Generation features:
- **Image Generation**: AI-powered image creation via Hugging Face models
- **Video Generation**: Text-to-video synthesis with multiple models
- **Music Generation**: AI music composition using Facebook's MusicGen
- **Model Flexibility**: Switch between different AI models
- **Local & API-Based**: Mix of local and cloud-based generation

## üñºÔ∏è Image Generation

### /imagine Command
Generate images from text descriptions.

**Syntax:**
```bash
/imagine <description>
```

**Examples:**
```bash
# Sci-fi scenes
/imagine a futuristic city skyline at sunset

# Character portraits
/imagine portrait of a cyberpunk hacker

# Abstract art
/imagine abstract digital art with purple and blue colors

# Landscapes
/imagine mountain landscape with aurora borealis

# Anime style
/imagine anime girl in school uniform
```

### How It Works
1. Text prompt sent to Hugging Face Inference API
2. Image generated by selected model
3. Image saved to `outputs/images/`
4. Image automatically displayed
5. Filename created from prompt

**Output:**
```
OPSIIE is dreaming... do not disturb.
Image generated and saved as 'outputs/images/futuristic_city_skyline_at_sunset.png'
```

### Model Management

**Check Current Model:**
```bash
/imagine model
```

**Output:**
```
Current dream engine model is: black-forest-labs/FLUX.1-dev
To change the model, use /imagine model <new_model_name>
```

**Change Model:**
```bash
/imagine model <model_name>
```

**Available Models:**
```bash
# FLUX (default - high quality)
/imagine model black-forest-labs/FLUX.1-dev

# Anime/Waifu style
/imagine model hakurei/waifu-diffusion

# NSFW capable (use responsibly)
/imagine model UnfilteredAI/NSFW-gen-v2

# Any Hugging Face model
/imagine model <username>/<model-name>
```

**Configuration:**
```python
# Default model
HF_API_URL = "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev"

# Requires Hugging Face API token in headers
```

### Image Generation Features
- **Auto-save**: Images saved to `outputs/images/`
- **Auto-display**: Images automatically opened
- **Clean filenames**: Prompt converted to filename
- **Format**: PNG (default)
- **API**: Hugging Face Inference API

### Error Handling
```
Error: The dream engine model is still loading
‚Üí Model needs warm-up time, retry in few minutes

Error: 503 Service Unavailable
‚Üí Model temporarily unavailable, try different model

Error: The content returned is not a valid image
‚Üí Invalid response from API, retry or change model
```

## üé¨ Video Generation

### /video Command
Generate videos from text descriptions.

**Syntax:**
```bash
/video <description>
```

**Examples:**
```bash
# Nature scenes
/video sunset timelapse over ocean waves

# Action sequences
/video cat playing with yarn ball

# Abstract visuals
/video abstract particles flowing in space

# Landscapes
/video mountain landscape with moving clouds
```

### How It Works
1. Text prompt processed by video diffusion model
2. Video frames generated (24 frames default)
3. Frames compiled into MP4 video
4. Video saved to `outputs/videos/`
5. Video automatically plays in browser

**Default Parameters:**
```python
{
    "num_frames": 24,
    "height": 256,
    "width": 256,
    "num_inference_steps": 20,
    "guidance_scale": 7.5
}
```

### Video Models

**Check Current Model:**
```bash
/video model
```

**Change Model:**
```bash
/video model <model_name>
```

**Available Models:**
```bash
# ModelScope (default)
/video model modelscope

# Zeroscope (higher quality)
/video model zeroscope

# VideoGen
/video model videogen

# TuneAVideo
/video model tuneavideo
```

**Model Details:**
```python
VIDEO_MODELS = {
    "modelscope": "damo-vilab/text-to-video-ms-1.7b",
    "zeroscope": "cerspense/zeroscope_v2_576w",
    "videogen": "VideoCrafter/videogen-1",
    "tuneavideo": "tuneavideo/stable-diffusion-v1-5-video"
}
```

### Video Generation Process
```
OPSIIE is dreaming... do not disturb.
[Generating 24 frames...]
[Compiling video...]
Video specimen generated and saved to: outputs/videos/sunset_timelapse.mp4
[Video plays in browser]
```

### Video Features
- **Resolution**: 256x256 (default, model-dependent)
- **Duration**: 24 frames (~1 second at 24fps)
- **Format**: MP4
- **Auto-play**: Opens in default browser
- **CUDA Support**: GPU acceleration if available

## üéµ Music Generation

### /music Command
Generate music from text descriptions.

**Syntax:**
```bash
/music <description>
```

**Examples:**
```bash
# Classical
/music calm piano with soft strings

# Electronic
/music electronic dance music with heavy bass

# Jazz
/music smooth jazz with saxophone

# Ambient
/music ambient soundscape for meditation

# Rock
/music energetic rock guitar solo
```

### How It Works
1. Text prompt processed by MusicGen model
2. Audio waveform generated
3. Audio saved as WAV file
4. Audio saved to `outputs/music/`
5. Audio automatically played

**Model:**
```python
# Facebook's MusicGen Small
musicgen_processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
musicgen_model = MusicgenForConditionalGeneration.from_pretrained(
    "facebook/musicgen-small", 
    attn_implementation="eager"
)
```

### Music Generation Process
```
OPSIIE is dreaming... do not disturb.
[Processing prompt...]
[Generating audio waveform...]
[Saving to WAV...]
Music specimen generated: outputs/music/calm_piano_with_soft_strings.wav
[Audio plays]
```

### Music Features
- **Model**: facebook/musicgen-small
- **Format**: WAV
- **Quality**: Model-dependent
- **Auto-play**: pygame mixer
- **CUDA Support**: GPU acceleration
- **Duration**: Configurable (model default)

## üé® Vision Analysis

### Image URL Analysis
OPSIIE can analyze images from URLs in conversations.

**How to Use:**
```
Provide image URL in your prompt, OPSIIE analyzes and describes it
```

**Example:**
```
You: What do you see in this image? https://example.com/image.jpg

OPSIIE: [Downloads image]
        [Analyzes with BLIP model]
        [Provides description]
        I see a futuristic cityscape with tall buildings...
```

### Vision Model
```python
# BLIP model for image captioning
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
```

**Features:**
- Automatic image download from URLs
- Image analysis and description
- Context integration in responses
- Supports common image formats

## üîß Technical Architecture

### Image Pipeline
```
Text Prompt ‚Üí HF API ‚Üí Model Processing ‚Üí Image Generation ‚Üí PNG Save ‚Üí Display
```

### Video Pipeline
```
Text Prompt ‚Üí Diffusion Model ‚Üí Frame Generation ‚Üí Video Compilation ‚Üí MP4 Save ‚Üí Browser Play
```

### Music Pipeline
```
Text Prompt ‚Üí MusicGen ‚Üí Waveform Generation ‚Üí WAV Save ‚Üí Audio Play
```

### Vision Pipeline
```
Image URL ‚Üí Download ‚Üí BLIP Model ‚Üí Description ‚Üí Context Integration
```

## üí° Best Practices

### Effective Prompts

**Images:**
```bash
# Be specific
/imagine cyberpunk hacker in neon-lit room with holographic displays

# Include style
/imagine oil painting of mountain landscape in impressionist style

# Specify details
/imagine portrait of woman with blue eyes, long hair, futuristic clothing
```

**Videos:**
```bash
# Describe motion
/video waves crashing on beach with sunset

# Be concise (video models prefer shorter prompts)
/video cat playing with ball

# Focus on main action
/video abstract particles swirling
```

**Music:**
```bash
# Specify genre and mood
/music upbeat jazz with piano and drums

# Include instruments
/music classical symphony with strings and brass

# Describe atmosphere
/music dark ambient with deep bass
```

### Resource Management

**GPU Usage:**
- Image generation: API-based (no local GPU needed)
- Video generation: Local GPU recommended
- Music generation: Local GPU recommended

**Storage:**
- Images: ~1-5 MB each
- Videos: ~5-20 MB each
- Music: ~2-10 MB each
- Monitor `outputs/` directory size

**Performance:**
- Image: ~10-30 seconds (API dependent)
- Video: ~1-5 minutes (GPU dependent)
- Music: ~30-60 seconds (GPU dependent)

## ‚öôÔ∏è Configuration

### API Keys
```env
# Hugging Face (for image generation)
HF_API_TOKEN=your_hugging_face_token
```

### Model Settings
```python
# Image model (changeable via command)
HF_API_URL = "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev"

# Video model (changeable via command)
current_model = "modelscope"

# Music model (fixed)
model = "facebook/musicgen-small"

# Vision model (fixed)
model = "Salesforce/blip-image-captioning-base"
```

### Output Directories
```python
ensure_directory_exists('outputs/images')
ensure_directory_exists('outputs/music')
ensure_directory_exists('outputs/videos')
```

## üö® Troubleshooting

### Image Generation Fails
```
Error: Model is loading (503)
‚Üí Wait a few minutes, retry

Error: Invalid API token
‚Üí Check HF_API_TOKEN in environment

Error: Rate limit exceeded
‚Üí Wait or use different model
```

### Video Generation Slow
```
Problem: Takes very long time
‚Üí Check GPU availability
‚Üí Try smaller model (modelscope vs zeroscope)
‚Üí Reduce num_frames parameter
```

### Music Generation Issues
```
Error: Model not initialized
‚Üí Check CUDA availability
‚Üí Verify model download
‚Üí Check disk space

Error: Audio playback failed
‚Üí Check audio drivers
‚Üí Verify pygame installation
```

### Vision Analysis Fails
```
Error: Could not download image
‚Üí Check URL validity
‚Üí Verify internet connection
‚Üí Check image format support
```

## üéØ Use Cases

### Creative Projects
```bash
# Generate concept art
/imagine futuristic vehicle design

# Create mood videos
/video abstract visuals for presentation

# Produce background music
/music ambient music for podcast intro
```

### Research & Development
```bash
# Visualize concepts
/imagine diagram of neural network architecture

# Create demo videos
/video simulation of particle movement

# Generate test audio
/music various musical styles for testing
```

### Content Creation
```bash
# Social media content
/imagine eye-catching thumbnail design

# Video content
/video intro sequence animation

# Podcast music
/music podcast outro music
```

## üìä Generation Statistics

Check system status:
```bash
/status
```

Shows:
- Generation system status
- Model availability
- GPU/CPU usage
- Output directory sizes

## üöÄ Advanced Usage

### Batch Generation
```bash
# Generate multiple images
/imagine concept 1
/imagine concept 2
/imagine concept 3

# Generate video sequences
/video scene 1
/video scene 2
/video scene 3
```

### Model Experimentation
```bash
# Try different image models
/imagine model FLUX.1-dev
/imagine futuristic city
/imagine model waifu-diffusion
/imagine anime character
/imagine model stable-diffusion-v1-5
/imagine realistic portrait
```

### Workflow Integration
```bash
# Create complete content
/imagine concept art for video game
/video game intro animation
/music epic game soundtrack

# All outputs saved to respective directories
```

## üìö Related Features

- **Voice Generation**: ElevenLabs for voice synthesis
- **File Processing**: Analyze generated images with `/read`
- **Memory**: Store generation prompts with `/memorize`
- **Rooms**: Collaborative generation with AI agents

---

**Multi-modal AI generation brings creativity to your fingertips.** üé®